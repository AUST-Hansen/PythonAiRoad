{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN的一种Spark实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，实现思路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，核心代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@532b1f81\n",
       "sc = org.apache.spark.SparkContext@5831e9b8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@5831e9b8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    ".builder()\n",
    ".appName(\"dbscan\")\n",
    ".enableHiveSupport()\n",
    ".getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext\n",
    "\n",
    "//以支持将RDD隐式转换成DataFrame\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1，寻找核心点形成临时聚类簇。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "该步骤一般要采用空间索引(如RTree)+广播的方法，此处从略，假定已经得到了临时聚类簇。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalArgumentException\n",
       "Message: Unsupported class file major version 56\n",
       "StackTrace:   at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n",
       "  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n",
       "  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n",
       "  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n",
       "  at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n",
       "  at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n",
       "  at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n",
       "  at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n",
       "  at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n",
       "  at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n",
       "  at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n",
       "  at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n",
       "  at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n",
       "  at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n",
       "  at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n",
       "  at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n",
       "  at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n",
       "  at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n",
       "  at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\n",
       "  at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307)\n",
       "  at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:392)\n",
       "  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306)\n",
       "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\n",
       "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2100)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//rdd_core的每一行代表一个临时聚类簇：(min_core_id, core_id_set)\n",
    "//core_id_set为临时聚类簇所有核心点的编号，min_core_id为这些编号中取值最小的编号\n",
    "var rdd_core = spark.sparkContext.parallelize(List((1L,Set(1L,2L)),(2L,Set(2L,3L,4L)),\n",
    "                                       (6L,Set(6L,8L,9L)),(4L,Set(4L,5L)),\n",
    "                                       (9L,Set(9L,10L,11L)),(15L,Set(15L,17L)),\n",
    "                                      (10L,Set(10L,11L,18L))))\n",
    "rdd_core.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2，合并临时聚类簇得到聚类簇。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//定义合并函数：将有共同核心点的临时聚类簇合并\n",
    "val mergeSets = (set_list: ListBuffer[Set[Long]]) =>{\n",
    "  var result = ListBuffer[Set[Long]]()\n",
    "  while (set_list.size>0){\n",
    "    var cur_set = set_list.remove(0)\n",
    "    var intersect_idxs = List.range(set_list.size-1,-1,-1).filter(i=>(cur_set&set_list(i)).size>0)\n",
    "    while(intersect_idxs.size>0){\n",
    "      for(idx<-intersect_idxs){\n",
    "        cur_set = cur_set|set_list(idx)\n",
    "      }\n",
    "      for(idx<-intersect_idxs){\n",
    "        set_list.remove(idx)\n",
    "      }\n",
    "      intersect_idxs = List.range(set_list.size-1,-1,-1).filter(i=>(cur_set&set_list(i)).size>0)\n",
    "    }\n",
    "    result = result:+cur_set\n",
    "  }\n",
    "  result\n",
    "}\n",
    "\n",
    "//对rdd_core分区后在每个分区对临时聚类簇合并，\n",
    "//不断将分区数量减少，最终合并到一个分区\n",
    "//rdd: (min_core_id,core_id_set)\n",
    "\n",
    "def mergeRDD(rdd: org.apache.spark.rdd.RDD[(Long,Set[Long])], partition_cnt:Int):\n",
    "org.apache.spark.rdd.RDD[(Long,Set[Long])] = {\n",
    "  val rdd_merged =  rdd.partitionBy(new HashPartitioner(partition_cnt))\n",
    "    .mapPartitions(iter => {\n",
    "      val buffer = ListBuffer[Set[Long]]()\n",
    "      for(t<-iter){\n",
    "        val core_id_set:Set[Long] = t._2\n",
    "        buffer.add(core_id_set)\n",
    "      }\n",
    "      val merged_buffer = mergeSets(buffer)\n",
    "      var result = List[(Long,Set[Long])]()\n",
    "      for(core_id_set<-merged_buffer){\n",
    "        val min_core_id = core_id_set.min\n",
    "        result = result:+(min_core_id,core_id_set)\n",
    "      }\n",
    "      result.iterator\n",
    "    })\n",
    "  rdd_merged\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_core = mergeRDD(rdd_core,8)\n",
    "rdd_core = mergeRDD(rdd_core,4)\n",
    "rdd_core = mergeRDD(rdd_core,1)\n",
    "rdd_core.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，完整范例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.locationtech.jts.geom.{Geometry, LineString,GeometryFactory,Polygon,Coordinate,Point}\n",
    "import org.locationtech.jts.index.strtree.STRtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
