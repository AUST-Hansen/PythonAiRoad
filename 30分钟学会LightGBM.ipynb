{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30分钟看懂LightGBM基本原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，LightGBM和XGBoost对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM可以看成是XGBoost的升级加强版本，2017年经微软推出后，便成为各种数据竞赛中刷分拿奖的神兵利器。\n",
    "\n",
    "正如其名字中的Light所蕴含的那样，和XGBoost相比，LightGBM在大规模数据集上跑起来更加轻盈。\n",
    "\n",
    "* 模型精度：XGBoost和LightGBM相当。\n",
    "\n",
    "* 训练速度：LightGBM远快于XGBoost。\n",
    "\n",
    "* 内存消耗：LightGBM远小于XGBoost。\n",
    "\n",
    "* 缺失值特征：XGBoost和LightGBM都可以自动处理特征缺失值。\n",
    "\n",
    "* 分类特征：XGBoost不支持类别特征，需要OneHot编码预处理。LightGBM直接支持类别特征。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/LightGBM&XGBoost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，LightGBM的性能优化原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM在XGBoost上主要有3方面的优化。\n",
    "\n",
    "1，Histogram算法:直方图加速算法。\n",
    "\n",
    "2，GOSS算法:基于梯度的单边采样算法。\n",
    "\n",
    "3，EFB算法:互斥特征捆绑算法。\n",
    "\n",
    "可以用如下一个简单公式来说明LightGBM和XGBoost的关系：\n",
    "\n",
    "LightGBM = XGBoost + Histogram + GOSS + EFB。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，Histogram算法，GOSS算法，和EFB算法分别从什么角度对XGBoost进行性能优化呢？\n",
    "\n",
    "我们先概括性地从全局进行分析，然后再逐个加以介绍。\n",
    "\n",
    "XGBoost模型训练的总体的复杂度可以粗略估计为：\n",
    "\n",
    "训练复杂度 = 树的棵数✖️每棵树上叶子的数量✖️生成每片叶子的复杂度\n",
    "\n",
    "由于XGBoost采用的基模型是二叉树，因此生成每片叶子需要分裂一次。\n",
    "\n",
    "而每次分裂，需要遍历所有特征上所有候选分裂点位，计算按照这些候选分裂点位分裂后的全部样本的目标函数增益，找到最大的那个增益对应的特征和候选分裂点位，从而生成一片新叶子。因此生成一片叶子的复杂度可以粗略估计为：\n",
    "\n",
    "生成一片叶子的复杂度 = 特征数量✖️候选分裂点位数量✖️样本的数量。\n",
    "\n",
    "而Hitogram算法的作用是减少候选分裂点位数量，GOSS算法的作用是减少样本的数量，EFB算法的作用是减少特征的数量。\n",
    "\n",
    "通过这3个算法的引入，LightGBM生成一片叶子需要的复杂度大大降低了，从而极大节约了计算时间。\n",
    "\n",
    "同时Histogram算法还将特征由浮点数转换成0~255位的整数进行存储，从而极大节约了内存存储。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1，Histogram算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直方图算法是替代XGBoost的预排序(pre-sorted)算法的。\n",
    "\n",
    "预排序算法首先将样本按照特征取值排序，然后从全部特征取值中找到最优的分裂点位，该算法的候选分裂点数量与样本数量成正比。\n",
    "\n",
    "而直方图算法通过将连续特征值离散化到固定数量(如255个)的bins上，使得候选分为点位为常数个(num_bins -1).\n",
    "\n",
    "此外，直方图算法还能够作直方图差加速。当节点分裂成两个时，右边叶子节点的直方图等于其父节点的直方图减去左边叶子节点的直方图。\n",
    "\n",
    "从而大大减少构建直方图的计算量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/预排序算法.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/直方图算法.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/直方图加速.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2，GOSS算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOSS算法全称为Gradient-based One-Side Sampling，即基于梯度的单边采样算法。\n",
    "\n",
    "其主要思想是通过对样本采样的方法来减少计算目标函数增益时候的复杂度。\n",
    "\n",
    "但如果对全部样本进行随机采样，势必会对目标函数增益的计算精度造成较大的影响。\n",
    "\n",
    "GOSS算法的创新之处在于它只对梯度绝对值较小的样本按照一定比例进行采样，而保留了梯度绝对值较大的样本。\n",
    "\n",
    "这就是所谓的单边采样。由于目标函数增益主要来自于梯度绝对值较大的样本，因此这种方法在计算性能和计算精度之间取得了很好的平衡。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/基于梯度的单边采样算法.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3，EFB算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EFB算法全称是Exclusive Feature Bundling，即互斥特征绑定算法。\n",
    "\n",
    "EFB算法可以有效减少用于构建直方图的特征数量，从而降低计算复杂度，尤其是特征中包含大量稀疏特征的时候。\n",
    "\n",
    "在许多应用场景下，数据集中会有大量的稀疏特征，这些稀疏特征大部分样本都取值为0，只有少数样本取值非0。\n",
    "\n",
    "通常可以认为这些稀疏特征是互斥的，即它们几乎不会同时取非零值。\n",
    "\n",
    "利用这种特性，可以通过对某些特征的取值重新编码，将多个这样互斥的特征捆绑成为一个新的特征。\n",
    "\n",
    "有趣的是，对于类别特征，如果转换成onehot编码，则这些onehot编码后的多个特征相互之间是互斥的，\n",
    "\n",
    "从而可以被捆绑成为一个特征。因此，对于指定为类别特征的特征，LightGBM可以直接将每个类别取值和一个bin关联，从而自动地处理它们。\n",
    "\n",
    "而无需预处理成onehot编码多此一举。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/互斥特征捆绑算法.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，LightGBM的使用范例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================2019-11-10 12:22:56\n",
      "step1: reading data......\n",
      "\n",
      "\n",
      "\n",
      "================================================================================2019-11-10 12:22:56\n",
      "step2: setting parameters......\n",
      "\n",
      "\n",
      "\n",
      "================================================================================2019-11-10 12:22:56\n",
      "step3: training model......\n",
      "\n",
      "\n",
      "[1]\ttrain's auc: 0.986257\tvalidate's auc: 0.955921\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttrain's auc: 0.986257\tvalidate's auc: 0.955921\n",
      "[3]\ttrain's auc: 0.988014\tvalidate's auc: 0.952632\n",
      "[4]\ttrain's auc: 0.987118\tvalidate's auc: 0.947368\n",
      "[5]\ttrain's auc: 0.988142\tvalidate's auc: 0.971053\n",
      "[6]\ttrain's auc: 0.99047\tvalidate's auc: 0.979934\n",
      "[7]\ttrain's auc: 0.991947\tvalidate's auc: 0.979605\n",
      "[8]\ttrain's auc: 0.992052\tvalidate's auc: 0.981031\n",
      "[9]\ttrain's auc: 0.992611\tvalidate's auc: 0.981031\n",
      "[10]\ttrain's auc: 0.992099\tvalidate's auc: 0.979825\n",
      "[11]\ttrain's auc: 0.992564\tvalidate's auc: 0.980482\n",
      "[12]\ttrain's auc: 0.993681\tvalidate's auc: 0.98136\n",
      "[13]\ttrain's auc: 0.993937\tvalidate's auc: 0.980921\n",
      "[14]\ttrain's auc: 0.994915\tvalidate's auc: 0.980044\n",
      "[15]\ttrain's auc: 0.994798\tvalidate's auc: 0.979167\n",
      "[16]\ttrain's auc: 0.995031\tvalidate's auc: 0.978947\n",
      "[17]\ttrain's auc: 0.995124\tvalidate's auc: 0.978728\n",
      "[18]\ttrain's auc: 0.995078\tvalidate's auc: 0.979167\n",
      "[19]\ttrain's auc: 0.995054\tvalidate's auc: 0.980044\n",
      "[20]\ttrain's auc: 0.994938\tvalidate's auc: 0.980482\n",
      "[21]\ttrain's auc: 0.995229\tvalidate's auc: 0.980482\n",
      "[22]\ttrain's auc: 0.995555\tvalidate's auc: 0.979825\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttrain's auc: 0.993681\tvalidate's auc: 0.98136\n",
      "\n",
      "================================================================================2019-11-10 12:22:57\n",
      "step4: evaluating model ......\n",
      "\n",
      "\n",
      "train accuracy: 0.96009 \n",
      "valid accuracy: 0.90909 \n",
      "\n",
      "\n",
      "================================================================================2019-11-10 12:22:57\n",
      "step5: saving model ......\n",
      "\n",
      "\n",
      "model_dir: data/gbm.model\n",
      "\n",
      "================================================================================2019-11-10 12:22:57\n",
      "task end......\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "    print(info+'...\\n\\n')\n",
    "\n",
    "#================================================================================\n",
    "# 一，读取数据\n",
    "#================================================================================\n",
    "printlog(\"step1: reading data...\")\n",
    "\n",
    "# 读取dftrain,dftest\n",
    "breast = datasets.load_breast_cancer()\n",
    "df = pd.DataFrame(breast.data,columns = [x.replace(' ','_') for x in breast.feature_names])\n",
    "df['label'] = breast.target\n",
    "df['mean_radius'] = df['mean_radius'].apply(lambda x:int(x))\n",
    "df['mean_texture'] = df['mean_texture'].apply(lambda x:int(x))\n",
    "dftrain,dftest = train_test_split(df)\n",
    "\n",
    "categorical_features = ['mean_radius','mean_texture']\n",
    "lgb_train = lgb.Dataset(dftrain.drop(['label'],axis = 1),label=dftrain['label'],\n",
    "                        categorical_feature = categorical_features)\n",
    "\n",
    "lgb_valid = lgb.Dataset(dftest.drop(['label'],axis = 1),label=dftest['label'],\n",
    "                        categorical_feature = categorical_features,\n",
    "                        reference=lgb_train)\n",
    "\n",
    "#================================================================================\n",
    "# 二，设置参数\n",
    "#================================================================================\n",
    "printlog(\"step2: setting parameters...\")\n",
    "                               \n",
    "boost_round = 50                   \n",
    "early_stop_rounds = 10\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective':'binary',\n",
    "    'metric': ['auc'],\n",
    "    'num_leaves': 31,   \n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "#================================================================================\n",
    "# 三，训练模型\n",
    "#================================================================================\n",
    "printlog(\"step3: training model...\")\n",
    "\n",
    "\n",
    "results = {}\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round= boost_round,\n",
    "                valid_sets=(lgb_valid, lgb_train),\n",
    "                valid_names=('validate','train'),\n",
    "                early_stopping_rounds = early_stop_rounds,\n",
    "                evals_result= results)\n",
    "\n",
    "#================================================================================\n",
    "# 四，评估模型\n",
    "#================================================================================\n",
    "printlog(\"step4: evaluating model ...\")\n",
    "\n",
    "y_pred_train = gbm.predict(dftrain.drop('label',axis = 1), num_iteration=gbm.best_iteration)\n",
    "y_pred_test = gbm.predict(dftest.drop('label',axis = 1), num_iteration=gbm.best_iteration)\n",
    "\n",
    "print('train accuracy: {:.5} '.format(accuracy_score(dftrain['label'],y_pred_train>0.5)))\n",
    "print('valid accuracy: {:.5} \\n'.format(accuracy_score(dftest['label'],y_pred_test>0.5)))\n",
    "\n",
    "lgb.plot_metric(results)\n",
    "lgb.plot_importance(gbm,importance_type = \"gain\")\n",
    "\n",
    "#================================================================================\n",
    "# 五，保存模型\n",
    "#================================================================================\n",
    "printlog(\"step5: saving model ...\")\n",
    "\n",
    "\n",
    "model_dir = \"data/gbm.model\"\n",
    "print(\"model_dir: %s\"%model_dir)\n",
    "gbm.save_model(\"data/gbm.model\")\n",
    "printlog(\"task end...\")\n",
    "\n",
    "###\n",
    "##\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x129517f98>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib auto\n",
    "lgb.plot_metric(results,metric = \"auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12961e0f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib auto\n",
    "lgb.plot_importance(gbm,importance_type = \"gain\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
